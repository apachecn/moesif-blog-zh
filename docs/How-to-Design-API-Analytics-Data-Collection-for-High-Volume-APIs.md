# 如何为高容量 API 设计 API 分析数据收集

> 原文：<https://www.moesif.com/blog/technical/scaling-analytics/How-to-Design-API-Analytics-Data-Collection-for-High-Volume-APIs/>

对于任何希望深入了解其 API 和平台使用情况的平台公司来说，API 分析平台都是关键。产品所有者、增长团队、开发者关系等可以利用这些见解，根据您平台业务的原始健康状况而非直觉做出更多战略决策。然而，许多 API 平台每天都有非常高的 API 调用量。当设计一个分析系统时，这种容量带来了一系列独特的挑战，该分析系统在扩展时不会削弱他们的平台，也不会在查看云供应商的账单时感到震惊。这篇文章深入探讨了我们如何设计 Moesif 的 API 分析平台来处理每天有数十亿次 API 调用的公司的一些内部工作。

## 代理/SDK

面向规模的设计从一个设计良好的代理开始，该代理从各种应用程序收集指标并发送到您的分析平台。由于代理或 SDK 嵌入在您的应用程序中，如果您的分析服务过载，设计不佳的代理或 SDK 可能会削弱您的核心服务。为了防止这种情况，您的代理应该有自动故障保险和终止开关。

首先，代理应该从您的核心应用程序代码中异步收集任何指标，并且应该位于带外，这样，即使您的分析服务长时间关闭，您的应用程序可用性也不会受到影响。确保处理指标数据中的任何错误都得到处理，并且不会传播回应用程序本身。因为您将异步收集指标，所以您将希望利用本地队列。这可以在内存或磁盘中完成，具体取决于所需的数据一致性。但是请记住，任何需要在本地分配的资源(比如队列所需的内存或磁盘)都需要受到约束。否则，如果您的分析服务长时间停机，队列可能会继续无限制地增长，最终导致进程耗尽内存并崩溃。解决这个问题的一种方法是通过固定队列大小和丢弃旧事件。

在网络断开期间丢弃旧事件意味着您牺牲了一致性来换取可用性，这是 Brewer 的 CAP 定理的一个关键支柱，在讨论分布式数据存储时通常会引用，但在这里也适用。排队还使您能够批量处理事件，从而减少网络开销。

更多信息，请阅读[为 API 构建 SDK 的最佳实践](/blog/technical/sdks/Best-Practices-for-Building-SDKs-for-APIs/)

## 抽样

如果您的 API 一天要处理数十亿个 API 调用，即使有排队和批处理，您也可能很快超过计算和存储需求。运行这样一个系统的计算成本很容易达到每年数十万或数百万美元。减少这种压力的一种方法是通过智能采样算法。虽然持续选择随机百分比的事件进行记录而不是丢弃的基本算法可能在短期内有效，但这可能会严重降低您的分析系统的有用性。这是因为大多数产品在客户使用时都呈现出幂律曲线。你的前 20%的顾客会创造你 80%的流量。这也意味着你有一个发送很少流量的长尾客户。大量抽样的客户最初发送的数据很少，这可能会对您的客户旅程产生扭曲的看法。

减少这一问题的一种方法是实施更智能的采样方案。一种方法是基于每个用户或每个 API 键进行采样。例如，前 20%客户的任何用户将具有 10%的抽样率，而其余客户将具有 100%的抽样率(即捕获所有活动)。这意味着您需要一种机制来将采样率传播到您的代理。

## 收集网络

无论您的分析服务是只能从您自己的基础架构访问，还是可以从内部网访问，您都应该有一个在其中一个收集节点关闭时能够处理故障转移的系统。一个设计良好的网络将利用全球多个数据中心之间基于 DNS 的负载平衡，如 AWS Route 53 或具有相对较短 TTL 的 Azure Traffic Manager。在每个数据中心内，您可以利用代理服务器(如 HAProxy、NGINX、AWS ELB)在您的收集器节点之间分配流量。我们 Moesif 的大量用户使用 HAProxy 在每个数据中心内进行负载平衡，使用 Azure 的流量管理器在不同的数据中心之间进行负载平衡。你的收集器逻辑应该很轻。它的工作应该只是认证，一些轻量级的输入验证，并尽快保存到磁盘上。这就是像卡夫卡这样的解决方案非常有用的地方。

## 汇总

即使您进行了采样，您也可能会发现您的存储成本在不断增加，并且查询会花费非常长的时间。像 Elasticsearch 和 Cassandra 这样的现代数据库可以通过其分布式架构保存数百 TB 的数据，但是当您试图对这些大规模数据集执行聚合时，您会遇到越来越长的查询(或者更糟:内存不足错误)。此外，存储需求持续增长。说到分析，您可以选择存储数据的分辨率。这可以是它们出现时的原始事件，也可以是在特定时段(如每分钟或每小时)发生的所有活动的汇总*故事*。汇总指标可以显著降低存储需求，但会牺牲指标分辨率。许多分析系统都有分层汇总策略。例如，过去 7 天的所有数据都存储到事件分辨率或每隔 5 秒存储一次。7 天前的旧数据可以汇总到每小时的时间间隔内。而即使是比一年前更冷的*数据也可能会以每天为间隔进行汇总。您不太可能查看精确到时间戳的事件*

## 度量估计

有时你不需要精确的计数，可以在一定的误差范围内生活。让我们以计算每周活跃 API 键为例。假设您有一个有序的 API 日志，其中包含请求时间戳和用于访问 API 的 API 键。一种简单的方法是创建一个按周索引的映射，然后为每个*桶*创建一个包含每个桶的唯一键的集合。这种方法的一个问题是，如果您有大量唯一的 API 键(也称为高基数)，您可能会很快耗尽内存。一种不同的方法是利用概率数据结构来推断或估计集合的唯一性，而不是直接计数。概率数据结构的例子包括 [HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog) 和 [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter) 毕竟，你可能会对上周是否有 100 或 10，000 个 API 键活跃感兴趣，但是如果你的分析系统报告 101 个而不是 100 个，你的决策也不会有所不同。对于这种情况，我们可以利用 HyperLogLog，它通过估计 API 键通过哈希算法哈希在一起时前导零的数量来估计唯一值的数量。准确性取决于为存储散列值所需的位向量分配了多少位。许多用于分析的现代数据库，如 Druid 和 Elasticsearch，都内置了 HyperLogLog，但能否确保您的数据能够利用它们取决于您。