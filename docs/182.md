# 以用户为中心的 API 指标与基础设施指标，以及如何选择正确的分析架构和数据存储

> 原文：<https://www.moesif.com/blog/technical/metrics/User-Centric-Metrics-vs-Infrastructure-Metrics-and-How-to-Choose-The-Right-Analytics-Architecture-and-Data-Store/>

在短短几年内，数据驱动的团队从没有足够的数据到淹没在指标的海洋中。现在可以跟踪、处理和分析每一个动作、反应和结果。然而，我们收到的一个关键问题是，哪些指标是重要的，哪种分析架构和数据存储最适合特定的分析要求。

对于技术产品和工程团队来说，有两种重要的度量标准:

*   基础设施指标，如 EPM(每分钟错误数)
*   以用户为中心的指标，如 MAU(每月活跃用户数)和会话时长

## 什么是产品指标？

API analytics 或 web analytics 之类的产品指标围绕着*用户*(或者*公司*，如果你是 B2B 的话)如何体验你的应用。也被称为*用户行为分析*，每一个事件或行为都可以追溯到单个客户，并且可以通过一起查看多个事件来发现行为趋势。

通过关注以客户为中心的使用，我们可以发现产品问题，例如为什么用户停止使用您的 API，或者他们最常使用的功能或端点。

产品指标牺牲了对后端基础架构的亚秒级可见性，同时提供了对跨多个会话和跨多个用户的单个用户的客户体验和行为的更多可见性。产品分析的一个关键区别在于，所有事件数据都与单个用户或公司相关联，这使得能够快速生成平均会话长度等聚合数据或生成采用漏斗。此外，产品指标通常会进行汇总，以支持更长期的趋势，如月环比或季度环比。

对于产品指标，您对某项服务在最后一分钟发生了什么不太感兴趣，而更感兴趣的是发现客户或产品体验的问题和长期趋势。客户不在乎单个请求是命中实例 A 还是实例 B，但是如果某个特性令人困惑或者没有创造价值，他们可能会停止使用您的服务。

Moesif、Amplitude 和 Mixpanel 等工具侧重于产品指标，包括:

*   *我的月活跃用户和 API 调用最多的用户是多少？*
*   *用户在我的采用漏斗中处于什么位置？*
*   哪些用户最忠诚？通过群组保持分析
*   哪些用户有糟糕的客户体验或可能流失？
*   *平均会话长度是多少？*

## 什么是基础设施指标？

基础设施指标不围绕用户。相反，它们围绕着内部服务的长期趋势。在查看基础设施指标时，您很少会跟踪用户实体。这类工具包括 Datadog、New Relic、Kibana 和其他 APM(应用程序性能监控)工具。

基础设施指标包括:

*   随着时间的推移，我的错误率和内存利用率趋势如何？
*   当服务失败时，过去 5 分钟内发生了什么？
*   *通过服务地图，哪些服务存在通信问题*

由于基础架构指标通常存储在基于时间的事件存储中，因此您可以进行超快速筛选和指标聚合，因为任何时间范围或服务/实例名称筛选都会显著减少数据集，例如只查看过去 24 小时或过去一小时内创建的数据。

## 比较基于时间的数据模型和以用户为中心的数据模型

如果您正在构建自己的度量系统，那么您选择的数据模型类型将对您可以对数据进行的分析类型产生重大影响。正如大多数工程师所知，事后更改数据模型是一项繁重的工作，可能需要迁移旧数据。

### 基于时间的数据存储

如果您想要搜索和分析日志以进行功能和性能监控，通常只添加基于时间的数据存储是最好的。由于基础架构指标最感兴趣的是现在发生的事情，或者过去 24 小时发生的事情，因此利用基于时间的模型可以将过去 24 小时的热数据存储在更强的节点上，而旧数据可以根据保留策略调出，甚至一起删除。基于时间的日志的一个巨大好处是，显示一段时间内趋势的聚合可以非常快地执行，因为聚合只涉及一小部分数据。任何指标聚合都可以通过简单的 map reduce 并行完成，这可以通过内部数据库流程或 Spark 等第三方框架来完成。

基于时间的指标的一个缺点是，通过一起查看多个事件来查看用户行为趋势几乎是不可能的。即使是最简单的查询，如查找平均会话长度，也会因为内存不足错误而削弱基于时间的存储，因为会话长度必须从每个用户的第一个和最后一个事件中获得。这是因为用户 id 是一个高基数的键，需要连接内存中每个用户的所有事件。除非您等待长时间运行的作业来加载和转换数据，否则无法完成漏斗和群组保留分析等更复杂的分析，这需要花费数小时或数天时间，从而消除了自助交互式分析的优势。

### 以用户为中心的数据存储

以用户为中心的数据存储不是基于时间的数据存储，而是由用户记录组织的。用户的所有关联事件数据都与该用户一起存储。
因此，与全局事件日志不同的是，您有了一个基于滚动期和用户 id 的两级树结构，并构建了*用户配置文件*，其中每个用户节点都与其各自的事件相关联。这使得为美国的所有用户执行诸如平均会话长度之类的计算变得相对琐碎，因为所有需要做的是对位于美国的用户进行过滤，然后执行简单的 map reduce 函数来计算每个用户的平均会话长度。完成后，我们可以将每个用户的会话长度整合成一个全局长度。不必担心用户 id 是高基数的。

当然，对于以用户为中心的数据存储，不利的一面是使数据过期或将特定时间段移动到更强大的节点会更加困难。虽然这是可行的，但滚动周期更长，例如每月或每年，以避免基于时间的索引的连接爆炸，而基于时间的索引可以滚动到每小时或每天。这意味着，即使你只对最近的数据感兴趣，比如上个小时发生的事情，你仍然会接触到比纯粹基于时间的方法更多的数据。当你关注周环比或月环比趋势时，以用户为中心的指数效果最佳。

第二个缺点是构建一个以用户为中心的管道来存储数据更加复杂。像 Logstash 和 Fluentd 这样的日志聚合管道将无法执行为正确的用户按顺序插入数据所需的洗牌和转换。因此，您必须利用 Spark 或 Hadoop 等分布式集群计算框架来转换数据，这需要更多的工程资源。执行 shuffle 的一种方法是首先将传入事件存储在基于时间的数据存储中，然后运行 Spark 作业。请记住，如果您的某个用户触发了大量事件，您可能会遇到降低性能的热点。